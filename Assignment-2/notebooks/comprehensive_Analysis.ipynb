{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb3c05b",
   "metadata": {},
   "source": [
    "## Part C — Comprehensive Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e0126",
   "metadata": {},
   "source": [
    "Purpose: Hyperparameter analysis and model comparison (Logistic, Softmax, Neural Network) on MNIST. This notebook reuses code from Part A and Part B provided by the student.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef9496",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "This notebook expects the DataLoaders train_loader, val_loader, test_loader (flattened 28x28 -> 784) to be defined in the environment. It also uses the provided model implementations (Logistic, Softmax, CustomFeedforwardNN) and training helpers.\n",
    "\n",
    "Run cells in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eeaf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import os , sys\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from src.logisitc_manual import LogisticRegression\n",
    "from src.softmax_manual import SoftmaxRegression\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b3c49",
   "metadata": {},
   "source": [
    "### Logistic Regression (Part A2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e782d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def binary_cross_entropy(y_pred, y_true):\n",
    "    eps = 1e-8\n",
    "    return -torch.mean(y_true * torch.log(y_pred + eps) + (1 - y_true) * torch.log(1 - y_pred + eps))\n",
    "\n",
    "def forward_pass(X_batch, W, b):\n",
    "    scores = X_batch @ W + b\n",
    "    y_pred = sigmoid(scores)\n",
    "    return y_pred\n",
    "\n",
    "def compute_accuracy(y_pred, y_true):\n",
    "    threshold = (y_pred >= 0.5)\n",
    "    predictions = threshold.float()\n",
    "    return (predictions.squeeze() == y_true).float().mean().item()\n",
    "\n",
    "def compute_gradients(X_batch, y_batch, y_pred):\n",
    "    error = y_pred - y_batch.unsqueeze(1)\n",
    "    dW = (X_batch.T @ error) / X_batch.shape[0]\n",
    "    db = error.mean()\n",
    "    return dW, db\n",
    "\n",
    "def update_weights(W, b, dW, db, alpha):\n",
    "    W -= alpha * dW\n",
    "    b -= alpha * db\n",
    "    return W, b\n",
    "\n",
    "def train_logistic(train_loader, val_loader, learning_rate=0.01, max_epochs=100, patience=10, min_delta=0.0005, early_stopping=True):\n",
    "    n_features = 28*28\n",
    "    W = torch.zeros(n_features, 1)\n",
    "    b = torch.zeros(1)\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    best_val_loss = float('inf'); epochs_no_improve = 0\n",
    "    best_W, best_b = None, None\n",
    "    for epoch in range(max_epochs):\n",
    "        train_loss_epoch = 0.0; train_acc_epoch = 0.0; n_train_batches = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            y_pred = forward_pass(X_batch, W, b)\n",
    "            loss = binary_cross_entropy(y_pred, y_batch.unsqueeze(1))\n",
    "            dW, db = compute_gradients(X_batch, y_batch, y_pred)\n",
    "            W, b = update_weights(W, b, dW, db, learning_rate)\n",
    "            train_loss_epoch += loss.item(); train_acc_epoch += compute_accuracy(y_pred, y_batch)\n",
    "            n_train_batches += 1\n",
    "        val_loss_epoch = 0.0; val_acc_epoch = 0.0; n_val = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                y_pred = forward_pass(X_batch, W, b)\n",
    "                loss = binary_cross_entropy(y_pred, y_batch.unsqueeze(1))\n",
    "                val_loss_epoch += loss.item(); val_acc_epoch += compute_accuracy(y_pred, y_batch)\n",
    "                n_val += 1\n",
    "        avg_train_loss = train_loss_epoch / n_train_batches\n",
    "        avg_val_loss = val_loss_epoch / n_val\n",
    "        train_losses.append(avg_train_loss); val_losses.append(avg_val_loss)\n",
    "        train_accs.append(train_acc_epoch / n_train_batches); val_accs.append(val_acc_epoch / n_val)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{max_epochs} - Train Loss: {train_losses[-1]:.4f} Train Acc: {train_accs[-1]:.4f} - Val Loss: {val_losses[-1]:.4f} Val Acc: {val_accs[-1]:.4f}\")\n",
    "        # early stopping\n",
    "        if early_stopping:\n",
    "            if avg_val_loss < best_val_loss - min_delta:\n",
    "                best_val_loss = avg_val_loss; epochs_no_improve = 0\n",
    "                best_W = W.clone(); best_b = b.clone()\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    if early_stopping and best_W is not None:\n",
    "        W = best_W; b = best_b\n",
    "    return W, b, train_losses, val_losses, train_accs, val_accs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f013855f",
   "metadata": {},
   "source": [
    "## Softmax Regression (Part A3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    def __init__(self, input_dim, num_classes, learning_rate=0.01, reg_lambda=0.0):\n",
    "        self.input_dim = input_dim; self.num_classes = num_classes; self.lr = learning_rate; self.reg_lambda = reg_lambda\n",
    "        self.W = torch.randn(self.input_dim, self.num_classes) * 0.01\n",
    "        self.b = torch.zeros(self.num_classes)\n",
    "        self.history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    def softmax(self, z):\n",
    "        exp_z = torch.exp(z - torch.max(z, dim=1, keepdim=True)[0])\n",
    "        return exp_z / torch.sum(exp_z, dim=1, keepdim=True)\n",
    "    def cross_entropy_loss(self, y_pred, y_true):\n",
    "        eps = 1e-8\n",
    "        y_true_one_hot = torch.nn.functional.one_hot(y_true.long(), num_classes=self.num_classes).float()\n",
    "        loss = -torch.mean(torch.sum(y_true_one_hot * torch.log(y_pred + eps), dim=1))\n",
    "        return loss\n",
    "    def predict_probabilities(self, X_batch):\n",
    "        scores = X_batch @ self.W + self.b\n",
    "        y_pred = self.softmax(scores)\n",
    "        return y_pred\n",
    "    def predict(self, X_batch):\n",
    "        probs = self.predict_probabilities(X_batch)\n",
    "        return torch.argmax(probs, dim=1)\n",
    "    def compute_accuracy(self, y_pred, y_true):\n",
    "        return torch.mean((y_pred == y_true).float())\n",
    "    def fit(self, train_loader, val_loader=None, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss, correct, total = 0.0, 0, 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                probs = self.predict_probabilities(X_batch)\n",
    "                loss = self.cross_entropy_loss(probs, y_batch)\n",
    "                y_true_one_hot = torch.nn.functional.one_hot(y_batch.long(), num_classes=self.num_classes).float()\n",
    "                grad_scores = (probs - y_true_one_hot) / X_batch.shape[0]\n",
    "                grad_W = X_batch.T @ grad_scores + self.reg_lambda * self.W\n",
    "                grad_b = torch.sum(grad_scores, dim=0)\n",
    "                self.W -= self.lr * grad_W\n",
    "                self.b -= self.lr * grad_b\n",
    "                total_loss += loss.item()\n",
    "                preds = torch.argmax(probs, dim=1)\n",
    "                correct += torch.sum(preds == y_batch).item()\n",
    "                total += y_batch.size(0)\n",
    "            train_loss = total_loss / len(train_loader); train_acc = correct / total\n",
    "            self.history['train_loss'].append(train_loss); self.history['train_acc'].append(train_acc)\n",
    "            if val_loader is not None:\n",
    "                val_loss, val_acc = self.evaluate(val_loader)\n",
    "                self.history['val_loss'].append(val_loss); self.history['val_acc'].append(val_acc)\n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {self.history['train_loss'][-1]:.4f}, Train Acc: {self.history['train_acc'][-1]:.4f} - Val Loss: {self.history['val_loss'][-1]:.4f}, Val Acc: {self.history['val_acc'][-1]:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    def evaluate(self, data_loader):\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in data_loader:\n",
    "                probs = self.predict_probabilities(X_batch)\n",
    "                loss = self.cross_entropy_loss(probs, y_batch)\n",
    "                total_loss += loss.item()\n",
    "                preds = torch.argmax(probs, dim=1)\n",
    "                correct += torch.sum(preds == y_batch).item()\n",
    "                total += y_batch.size(0)\n",
    "        return total_loss / len(data_loader), correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95811e0d",
   "metadata": {},
   "source": [
    "\n",
    "## Neural network (Part B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7fccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden1_size=128, hidden2_size=64, output_size=10):\n",
    "        super(CustomFeedforwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        nn.init.zeros_(self.fc3.bias)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Training loop for neural network (Part B)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model_once(model, train_loader, val_loader, epochs=10, learning_rate=0.01, device=device):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} Train Loss: {train_loss:.4f} Train Acc: {train_acc:.4f} Val Loss: {val_loss:.4f} Val Acc: {val_acc:.4f}\")\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "# Multiple-run helper to compute mean +/- std\n",
    "def train_multiple_times(model_class, train_loader, val_loader, epochs=10, learning_rate=0.01, runs=5):\n",
    "    all_train_losses, all_val_losses = [], []\n",
    "    all_train_acc, all_val_acc = [], []\n",
    "    for r in range(runs):\n",
    "        print(f\"\\nRun {r+1}/{runs}\")\n",
    "        model = model_class()\n",
    "        tl, vl, ta, va = train_model_once(model, train_loader, val_loader, epochs, learning_rate, device)\n",
    "        all_train_losses.append(tl); all_val_losses.append(vl)\n",
    "        all_train_acc.append(ta); all_val_acc.append(va)\n",
    "    all_train_losses = np.array(all_train_losses)\n",
    "    all_val_losses = np.array(all_val_losses)\n",
    "    all_train_acc = np.array(all_train_acc)\n",
    "    all_val_acc = np.array(all_val_acc)\n",
    "    mean_train_loss = np.mean(all_train_losses, axis=0)\n",
    "    std_train_loss = np.std(all_train_losses, axis=0)\n",
    "    mean_val_loss = np.mean(all_val_losses, axis=0)\n",
    "    std_val_loss = np.std(all_val_losses, axis=0)\n",
    "    mean_train_acc = np.mean(all_train_acc, axis=0)\n",
    "    std_train_acc = np.std(all_train_acc, axis=0)\n",
    "    mean_val_acc = np.mean(all_val_acc, axis=0)\n",
    "    std_val_acc = np.std(all_val_acc, axis=0)\n",
    "    epochs_range = range(1, epochs+1)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.errorbar(epochs_range, mean_train_loss, yerr=std_train_loss, label='Training Loss')\n",
    "    plt.errorbar(epochs_range, mean_val_loss, yerr=std_val_loss, label='Validation Loss')\n",
    "    plt.title(\"Loss over Epochs with Error Bars\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(); plt.show()\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.errorbar(epochs_range, mean_train_acc, yerr=std_train_acc, label='Training Acc')\n",
    "    plt.errorbar(epochs_range, mean_val_acc, yerr=std_val_acc, label='Validation Acc')\n",
    "    plt.title(\"Accuracy over Epochs with Error Bars\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(); plt.show()\n",
    "    val_loss_diff = np.abs(np.diff(mean_val_loss))\n",
    "    convergence_epoch = np.argmin(val_loss_diff) + 1 if len(val_loss_diff)>0 else 1\n",
    "    print(f\"Convergence epoch (approx): {convergence_epoch}\")\n",
    "    return {\n",
    "        'mean_train_loss': mean_train_loss, 'std_train_loss': std_train_loss,\n",
    "        'mean_val_loss': mean_val_loss, 'std_val_loss': std_val_loss,\n",
    "        'mean_train_acc': mean_train_acc, 'std_train_acc': std_train_acc,\n",
    "        'mean_val_acc': mean_val_acc, 'std_val_acc': std_val_acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f00348",
   "metadata": {},
   "source": [
    "## C1 — Hyperparameter Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67071756",
   "metadata": {},
   "source": [
    "### C1.1 Learning Rate Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "results_lr = []\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n=== LR = {lr} ===\")\n",
    "    # ensure we use DataLoaders with the chosen batch size\n",
    "    # If your original train_loader has different batch size, recreate it from tensors if available\n",
    "    # Here we assume train_loader and val_loader accept any batch sizes; if not, recreate from TensorDataset.\n",
    "    model = CustomFeedforwardNN()\n",
    "    tl, vl, ta, va = train_model_once(model, train_loader, val_loader, epochs=num_epochs, learning_rate=lr)\n",
    "    results_lr.append({'lr': lr, 'train_losses': tl, 'val_losses': vl, 'train_acc': ta, 'val_acc': va})\n",
    "\n",
    "# Plot validation loss comparison\n",
    "plt.figure(figsize=(8,5))\n",
    "for res in results_lr:\n",
    "    plt.plot(res['val_losses'], label=f\"lr={res['lr']}\")\n",
    "plt.xlabel('Epoch'); plt.ylabel('Val Loss'); plt.title('Validation Loss for different learning rates'); plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "# Summarize final val accuracy\n",
    "pd.DataFrame([{'lr': r['lr'], 'final_val_acc': r['val_acc'][-1]} for r in results_lr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac1218",
   "metadata": {},
   "source": [
    "### C1.2 Batch Size Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [16, 32, 64, 128]\n",
    "num_epochs = 20\n",
    "results_bs = []\n",
    "# If tensors for X_train_flat etc exist, recreate DataLoaders with different batch sizes\n",
    "try:\n",
    "    X_train_flat\n",
    "    recreate_loaders = True\n",
    "except NameError:\n",
    "    recreate_loaders = False\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    print(f\"\\n=== Batch size = {bs} ===\")\n",
    "    if recreate_loaders:\n",
    "        train_loader_bs = DataLoader(TensorDataset(X_train_flat, y_train), batch_size=bs, shuffle=True)\n",
    "        val_loader_bs = DataLoader(TensorDataset(X_val_flat, y_val), batch_size=bs, shuffle=False)\n",
    "    else:\n",
    "        train_loader_bs = train_loader; val_loader_bs = val_loader\n",
    "    model = CustomFeedforwardNN()\n",
    "    start = time.time()\n",
    "    tl, vl, ta, va = train_model_once(model, train_loader_bs, val_loader_bs, epochs=num_epochs, learning_rate=0.01)\n",
    "    duration = time.time() - start\n",
    "    results_bs.append({'bs': bs, 'train_losses': tl, 'val_losses': vl, 'train_acc': ta, 'val_acc': va, 'time_s': duration})\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "for res in results_bs:\n",
    "    plt.plot(res['val_losses'], label=f\"bs={res['bs']}\")\n",
    "plt.xlabel('Epoch'); plt.ylabel('Val Loss'); plt.title('Validation Loss for different batch sizes'); plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "pd.DataFrame([{'batch_size': r['bs'], 'final_val_acc': r['val_acc'][-1], 'time_s': r['time_s']} for r in results_bs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94df81a",
   "metadata": {},
   "source": [
    "### C1.3 Architecture Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c14257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ffnn(hidden_sizes):\n",
    "    # hidden_sizes: list of ints\n",
    "    if len(hidden_sizes) == 1:\n",
    "        return CustomFeedforwardNN(input_size=784, hidden1_size=hidden_sizes[0], hidden2_size=64, output_size=10)\n",
    "    elif len(hidden_sizes) == 2:\n",
    "        return CustomFeedforwardNN(input_size=784, hidden1_size=hidden_sizes[0], hidden2_size=hidden_sizes[1], output_size=10)\n",
    "    else:\n",
    "        # for >2 layers create dynamic module\n",
    "        class FFN_dynamic(nn.Module):\n",
    "            def __init__(self, input_dim=784, hidden_sizes=[128,64], output_dim=10):\n",
    "                super().__init__()\n",
    "                layers = []\n",
    "                prev = input_dim\n",
    "                for h in hidden_sizes:\n",
    "                    layers.append(nn.Linear(prev, h))\n",
    "                    layers.append(nn.ReLU())\n",
    "                    prev = h\n",
    "                layers.append(nn.Linear(prev, output_dim))\n",
    "                self.net = nn.Sequential(*layers)\n",
    "                for m in self.net:\n",
    "                    if isinstance(m, nn.Linear):\n",
    "                        nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
    "            def forward(self, x):\n",
    "                return self.net(x)\n",
    "        return FFN_dynamic(input_dim=784, hidden_sizes=hidden_sizes, output_dim=10)\n",
    "\n",
    "layers_options = [ [64,64], [128,64], [256,128,64], [512,256,128,64] ]\n",
    "results_arch = []\n",
    "for hidden in layers_options:\n",
    "    print(f\"\\n=== Arch: {hidden} ===\")\n",
    "    model = make_ffnn(hidden)\n",
    "    tl, vl, ta, va = train_model_once(model, train_loader, val_loader, epochs=20, learning_rate=0.01)\n",
    "    results_arch.append({'hidden': hidden, 'train_acc': ta[-1], 'val_acc': va[-1], 'train_losses': tl, 'val_losses': vl})\n",
    "\n",
    "pd.DataFrame([{'architecture': str(r['hidden']), 'train_acc': r['train_acc'], 'val_acc': r['val_acc']} for r in results_arch])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e9ebfc",
   "metadata": {},
   "source": [
    "## C2 — Model Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a17133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Regression (multiclass)\n",
    "soft_model = SoftmaxRegression(input_dim=784, num_classes=10, learning_rate=0.1)\n",
    "soft_model.fit(train_loader, val_loader, epochs=50)\n",
    "val_loss_soft, val_acc_soft = soft_model.evaluate(val_loader)\n",
    "print(\"Softmax val acc:\", val_acc_soft)\n",
    "\n",
    "# Neural network: choose best from architecture experiments (example pick index 1)\n",
    "best_hidden = results_arch[1]['hidden'] if len(results_arch)>1 else [128,64]\n",
    "best_model = make_ffnn(best_hidden)\n",
    "start = time.time()\n",
    "tl, vl, ta, va = train_model_once(best_model, train_loader, val_loader, epochs=30, learning_rate=0.01)\n",
    "nn_time = time.time() - start\n",
    "# Evaluate on test set utility\n",
    "\n",
    "def evaluate_nn_on_loader(model, loader):\n",
    "    model.to(device); model.eval()\n",
    "    correct = 0; total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            correct += (preds.cpu() == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    return correct/total, np.array(y_true), np.array(y_pred)\n",
    "\n",
    "# Evaluate Softmax on test\n",
    "soft_test_loss, soft_test_acc = soft_model.evaluate(test_loader)\n",
    "print('Softmax test acc:', soft_test_acc)\n",
    "\n",
    "# Evaluate NN on test\n",
    "nn_test_acc, y_true_nn, y_pred_nn = evaluate_nn_on_loader(best_model, test_loader)\n",
    "print('NN test acc:', nn_test_acc)\n",
    "\n",
    "# Logistic: if binary problem, train logistic regression\n",
    "# If you want to compare on full 10-class, logistic is not directly used (would require one-vs-rest); we keep logistic for binary experiments.\n",
    "\n",
    "# Summary table\n",
    "comparison = pd.DataFrame([\n",
    "    {'Model': 'Softmax Regression', 'Test Accuracy': float(soft_test_acc), 'Training Time (s)': float('nan')},\n",
    "    {'Model': f'Neural Network {best_hidden}', 'Test Accuracy': float(nn_test_acc), 'Training Time (s)': nn_time}\n",
    "])\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e36a8",
   "metadata": {},
   "source": [
    "## C2.1 Confusion Matrix & Misclassified Examples (Best NN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdee0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true_nn, y_pred_nn)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "disp.plot(ax=ax)\n",
    "plt.title('Confusion Matrix - Best NN')\n",
    "plt.show()\n",
    "\n",
    "# Show some misclassified examples\n",
    "mis_idx = np.where(y_true_nn != y_pred_nn)[0]\n",
    "print('Total misclassified:', len(mis_idx))\n",
    "\n",
    "# Grab images from test dataset (original test dataset should be available as X_test / y_test or test_loader.dataset)\n",
    "# Assuming test_loader.dataset is a TensorDataset of flattened images\n",
    "\n",
    "dataset_for_vis = test_loader.dataset\n",
    "fig, axes = plt.subplots(2,5, figsize=(12,5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i >= len(mis_idx): break\n",
    "    idx = mis_idx[i]\n",
    "    # if dataset stores flattened images, reshape\n",
    "    img_flat, true_label = dataset_for_vis[idx]\n",
    "    if img_flat.ndim == 1:\n",
    "        img = img_flat.reshape(28,28)\n",
    "    else:\n",
    "        img = img_flat.squeeze()\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f\"Pred: {y_pred_nn[idx]} / True: {y_true_nn[idx]}\")\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5363bacc",
   "metadata": {},
   "source": [
    "## C3 — Final Evaluation & Retraining Best Model on Train+Val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c29a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train + val into one dataset and retrain best model\n",
    "try:\n",
    "    X_train_flat; X_val_flat; y_train; y_val\n",
    "    X_train_comb = torch.cat([X_train_flat, X_val_flat], dim=0)\n",
    "    y_train_comb = torch.cat([y_train, y_val], dim=0)\n",
    "    combined_loader = DataLoader(TensorDataset(X_train_comb, y_train_comb), batch_size=64, shuffle=True)\n",
    "    final_model = make_ffnn(best_hidden)\n",
    "    # train\n",
    "    train_model_once(final_model, combined_loader, test_loader, epochs=30, learning_rate=0.01)\n",
    "    final_acc, y_true_final, y_pred_final = evaluate_nn_on_loader(final_model, test_loader)\n",
    "    print('Final test acc after retraining on train+val:', final_acc)\n",
    "except NameError:\n",
    "    print('Train/Val tensors not found in workspace; skip final retrain step or recreate tensors.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb02b0ab",
   "metadata": {},
   "source": [
    "## Save results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model weights (optional)\n",
    "try:\n",
    "    torch.save(final_model.state_dict(), 'best_model_final.pt')\n",
    "    print('Saved best_model_final.pt')\n",
    "except NameError:\n",
    "    print('Final model not trained — no file saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c7a2c",
   "metadata": {},
   "source": [
    "## End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
