{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb3c05b",
   "metadata": {},
   "source": [
    "## Part C — Comprehensive Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e0126",
   "metadata": {},
   "source": [
    "Purpose: Hyperparameter analysis and model comparison (Logistic, Softmax, Neural Network) on MNIST. This notebook reuses code from Part A and Part B provided by the student.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef9496",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "This notebook expects the DataLoaders train_loader, val_loader, test_loader (flattened 28x28 -> 784) to be defined in the environment. It also uses the provided model implementations (Logistic, Softmax, CustomFeedforwardNN) and training helpers.\n",
    "\n",
    "Run cells in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eeaf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import os , sys\n",
    "## uncomment this if you are running on vs code\n",
    "# sys.path.append(os.path.abspath(\"../\"))\n",
    "# from src.logisitc_manual import LogisticRegression\n",
    "# from src.softmax_manual import SoftmaxRegression\n",
    "# from src.nn_manual import *\n",
    "# from src.data_preprocessing import *\n",
    "\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa58f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment if you are on vs code\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# from logisitc_manual import LogisticRegression\n",
    "# from softmax_manual import SoftmaxRegression\n",
    "# from data_preprocessing import *\n",
    "# from nn_manual import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f51bc",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5703377",
   "metadata": {},
   "source": [
    "### For Logisitc Regression (Class 0 vs 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1c13df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data = MNISTDataLoader(batch_size=32, binary=True, digits=(0, 1))\n",
    "train_loader_bin, val_loader_bin, test_loader_bin = binary_data.get_loaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8526ca55",
   "metadata": {},
   "source": [
    "## For Softmax Regression and NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f67bf9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_data = MNISTDataLoader(batch_size=32, binary=False)\n",
    "train_loader, val_loader, test_loader = multi_data.get_loaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f00348",
   "metadata": {},
   "source": [
    "## C1 — Hyperparameter Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67071756",
   "metadata": {},
   "source": [
    "### C1.1 Learning Rate Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28bb1377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR = 0.001 ===\n",
      "Epoch 1/20 Train Loss: 2.1038 Train Acc: 0.3474 Val Loss: 1.8041 Val Acc: 0.6135\n",
      "Epoch 2/20 Train Loss: 1.4583 Train Acc: 0.6974 Val Loss: 1.1391 Val Acc: 0.7711\n",
      "Epoch 3/20 Train Loss: 0.9309 Train Acc: 0.8017 Val Loss: 0.7727 Val Acc: 0.8245\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# ensure we use DataLoaders with the chosen batch size\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# If your original train_loader has different batch size, recreate it from tensors if available\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Here we assume train_loader and val_loader accept any batch sizes; if not, recreate from TensorDataset.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     model \u001b[38;5;241m=\u001b[39m CustomFeedforwardNN()\n\u001b[1;32m---> 11\u001b[0m     tl, vl, ta, va \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     results_lr\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: lr, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_losses\u001b[39m\u001b[38;5;124m'\u001b[39m: tl, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_losses\u001b[39m\u001b[38;5;124m'\u001b[39m: vl, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: ta, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: va})\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Plot validation loss comparison\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\habib\\Desktop\\TERM 7\\ML\\ML-Assignments-25\\Assignment-2\\src\\nn_manual.py:64\u001b[0m, in \u001b[0;36mtrain_model_once\u001b[1;34m(model, train_loader, val_loader, epochs, learning_rate, device)\u001b[0m\n\u001b[0;32m     62\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()        \u001b[38;5;66;03m# update model weights\u001b[39;00m\n\u001b[0;32m     63\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# accumulate total loss (weighted by batch size)\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# calculate predicted class labels\u001b[39;00m\n\u001b[0;32m     65\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# total samples processed\u001b[39;00m\n\u001b[0;32m     66\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# calculate correct predictions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "results_lr = []\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n=== LR = {lr} ===\")\n",
    "    # ensure we use DataLoaders with the chosen batch size\n",
    "    # If your original train_loader has different batch size, recreate it from tensors if available\n",
    "    # Here we assume train_loader and val_loader accept any batch sizes; if not, recreate from TensorDataset.\n",
    "    model = CustomFeedforwardNN()\n",
    "    tl, vl, ta, va = train_model_once(model, train_loader, val_loader, epochs=num_epochs, learning_rate=lr)\n",
    "    results_lr.append({'lr': lr, 'train_losses': tl, 'val_losses': vl, 'train_acc': ta, 'val_acc': va})\n",
    "\n",
    "# Plot validation loss comparison\n",
    "plt.figure(figsize=(8,5))\n",
    "for res in results_lr:\n",
    "    plt.plot(res['val_losses'], label=f\"lr={res['lr']}\")\n",
    "plt.xlabel('Epoch'); plt.ylabel('Val Loss'); plt.title('Validation Loss for different learning rates'); plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "# Summarize final val accuracy\n",
    "pd.DataFrame([{'lr': r['lr'], 'final_val_acc': r['val_acc'][-1]} for r in results_lr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac1218",
   "metadata": {},
   "source": [
    "### C1.2 Batch Size Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [16, 32, 64, 128]\n",
    "num_epochs = 20\n",
    "results_bs = []\n",
    "# If tensors for X_train_flat etc exist, recreate DataLoaders with different batch sizes\n",
    "try:\n",
    "    X_train_flat\n",
    "    recreate_loaders = True\n",
    "except NameError:\n",
    "    recreate_loaders = False\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    print(f\"\\n=== Batch size = {bs} ===\")\n",
    "    if recreate_loaders:\n",
    "        train_loader_bs = DataLoader(TensorDataset(X_train_flat, y_train), batch_size=bs, shuffle=True)\n",
    "        val_loader_bs = DataLoader(TensorDataset(X_val_flat, y_val), batch_size=bs, shuffle=False)\n",
    "    else:\n",
    "        train_loader_bs = train_loader; val_loader_bs = val_loader\n",
    "    model = CustomFeedforwardNN()\n",
    "    start = time.time()\n",
    "    tl, vl, ta, va = train_model_once(model, train_loader_bs, val_loader_bs, epochs=num_epochs, learning_rate=0.01)\n",
    "    duration = time.time() - start\n",
    "    results_bs.append({'bs': bs, 'train_losses': tl, 'val_losses': vl, 'train_acc': ta, 'val_acc': va, 'time_s': duration})\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "for res in results_bs:\n",
    "    plt.plot(res['val_losses'], label=f\"bs={res['bs']}\")\n",
    "plt.xlabel('Epoch'); plt.ylabel('Val Loss'); plt.title('Validation Loss for different batch sizes'); plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "pd.DataFrame([{'batch_size': r['bs'], 'final_val_acc': r['val_acc'][-1], 'time_s': r['time_s']} for r in results_bs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94df81a",
   "metadata": {},
   "source": [
    "### C1.3 Architecture Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c14257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ffnn(hidden_sizes):\n",
    "    # hidden_sizes: list of ints\n",
    "    if len(hidden_sizes) == 1:\n",
    "        return CustomFeedforwardNN(input_size=784, hidden1_size=hidden_sizes[0], hidden2_size=64, output_size=10)\n",
    "    elif len(hidden_sizes) == 2:\n",
    "        return CustomFeedforwardNN(input_size=784, hidden1_size=hidden_sizes[0], hidden2_size=hidden_sizes[1], output_size=10)\n",
    "    else:\n",
    "        # for >2 layers create dynamic module\n",
    "        class FFN_dynamic(nn.Module):\n",
    "            def __init__(self, input_dim=784, hidden_sizes=[128,64], output_dim=10):\n",
    "                super().__init__()\n",
    "                layers = []\n",
    "                prev = input_dim\n",
    "                for h in hidden_sizes:\n",
    "                    layers.append(nn.Linear(prev, h))\n",
    "                    layers.append(nn.ReLU())\n",
    "                    prev = h\n",
    "                layers.append(nn.Linear(prev, output_dim))\n",
    "                self.net = nn.Sequential(*layers)\n",
    "                for m in self.net:\n",
    "                    if isinstance(m, nn.Linear):\n",
    "                        nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
    "            def forward(self, x):\n",
    "                return self.net(x)\n",
    "        return FFN_dynamic(input_dim=784, hidden_sizes=hidden_sizes, output_dim=10)\n",
    "\n",
    "layers_options = [ [64,64], [128,64], [256,128,64], [512,256,128,64] ]\n",
    "results_arch = []\n",
    "for hidden in layers_options:\n",
    "    print(f\"\\n=== Arch: {hidden} ===\")\n",
    "    model = make_ffnn(hidden)\n",
    "    tl, vl, ta, va = train_model_once(model, train_loader, val_loader, epochs=20, learning_rate=0.01)\n",
    "    results_arch.append({'hidden': hidden, 'train_acc': ta[-1], 'val_acc': va[-1], 'train_losses': tl, 'val_losses': vl})\n",
    "\n",
    "pd.DataFrame([{'architecture': str(r['hidden']), 'train_acc': r['train_acc'], 'val_acc': r['val_acc']} for r in results_arch])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e9ebfc",
   "metadata": {},
   "source": [
    "## C2 — Model Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a17133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Regression (multiclass)\n",
    "soft_model = SoftmaxRegression(input_dim=784, num_classes=10, learning_rate=0.1)\n",
    "soft_model.fit(train_loader, val_loader, epochs=50)\n",
    "val_loss_soft, val_acc_soft = soft_model.evaluate(val_loader)\n",
    "print(\"Softmax val acc:\", val_acc_soft)\n",
    "\n",
    "# Neural network: choose best from architecture experiments (example pick index 1)\n",
    "best_hidden = results_arch[1]['hidden'] if len(results_arch)>1 else [128,64]\n",
    "best_model = make_ffnn(best_hidden)\n",
    "start = time.time()\n",
    "tl, vl, ta, va = train_model_once(best_model, train_loader, val_loader, epochs=30, learning_rate=0.01)\n",
    "nn_time = time.time() - start\n",
    "# Evaluate on test set utility\n",
    "\n",
    "def evaluate_nn_on_loader(model, loader):\n",
    "    model.to(device); model.eval()\n",
    "    correct = 0; total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            correct += (preds.cpu() == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    return correct/total, np.array(y_true), np.array(y_pred)\n",
    "\n",
    "# Evaluate Softmax on test\n",
    "soft_test_loss, soft_test_acc = soft_model.evaluate(test_loader)\n",
    "print('Softmax test acc:', soft_test_acc)\n",
    "\n",
    "# Evaluate NN on test\n",
    "nn_test_acc, y_true_nn, y_pred_nn = evaluate_nn_on_loader(best_model, test_loader)\n",
    "print('NN test acc:', nn_test_acc)\n",
    "\n",
    "# Logistic: if binary problem, train logistic regression\n",
    "# If you want to compare on full 10-class, logistic is not directly used (would require one-vs-rest); we keep logistic for binary experiments.\n",
    "\n",
    "# Summary table\n",
    "comparison = pd.DataFrame([\n",
    "    {'Model': 'Softmax Regression', 'Test Accuracy': float(soft_test_acc), 'Training Time (s)': float('nan')},\n",
    "    {'Model': f'Neural Network {best_hidden}', 'Test Accuracy': float(nn_test_acc), 'Training Time (s)': nn_time}\n",
    "])\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e36a8",
   "metadata": {},
   "source": [
    "## C2.1 Confusion Matrix & Misclassified Examples (Best NN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdee0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true_nn, y_pred_nn)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "disp.plot(ax=ax)\n",
    "plt.title('Confusion Matrix - Best NN')\n",
    "plt.show()\n",
    "\n",
    "# Show some misclassified examples\n",
    "mis_idx = np.where(y_true_nn != y_pred_nn)[0]\n",
    "print('Total misclassified:', len(mis_idx))\n",
    "\n",
    "# Grab images from test dataset (original test dataset should be available as X_test / y_test or test_loader.dataset)\n",
    "# Assuming test_loader.dataset is a TensorDataset of flattened images\n",
    "\n",
    "dataset_for_vis = test_loader.dataset\n",
    "fig, axes = plt.subplots(2,5, figsize=(12,5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i >= len(mis_idx): break\n",
    "    idx = mis_idx[i]\n",
    "    # if dataset stores flattened images, reshape\n",
    "    img_flat, true_label = dataset_for_vis[idx]\n",
    "    if img_flat.ndim == 1:\n",
    "        img = img_flat.reshape(28,28)\n",
    "    else:\n",
    "        img = img_flat.squeeze()\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f\"Pred: {y_pred_nn[idx]} / True: {y_true_nn[idx]}\")\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5363bacc",
   "metadata": {},
   "source": [
    "## C3 — Final Evaluation & Retraining Best Model on Train+Val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c29a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train + val into one dataset and retrain best model\n",
    "try:\n",
    "    X_train_flat; X_val_flat; y_train; y_val\n",
    "    X_train_comb = torch.cat([X_train_flat, X_val_flat], dim=0)\n",
    "    y_train_comb = torch.cat([y_train, y_val], dim=0)\n",
    "    combined_loader = DataLoader(TensorDataset(X_train_comb, y_train_comb), batch_size=64, shuffle=True)\n",
    "    final_model = make_ffnn(best_hidden)\n",
    "    # train\n",
    "    train_model_once(final_model, combined_loader, test_loader, epochs=30, learning_rate=0.01)\n",
    "    final_acc, y_true_final, y_pred_final = evaluate_nn_on_loader(final_model, test_loader)\n",
    "    print('Final test acc after retraining on train+val:', final_acc)\n",
    "except NameError:\n",
    "    print('Train/Val tensors not found in workspace; skip final retrain step or recreate tensors.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb02b0ab",
   "metadata": {},
   "source": [
    "## Save results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model weights (optional)\n",
    "try:\n",
    "    torch.save(final_model.state_dict(), 'best_model_final.pt')\n",
    "    print('Saved best_model_final.pt')\n",
    "except NameError:\n",
    "    print('Final model not trained — no file saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c7a2c",
   "metadata": {},
   "source": [
    "## End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
